---------------1 TF,Pytorch----------------------
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

# Generate some sample data
np.random.seed(42)
X_train = np.random.rand(1000, 10)
y_train = np.sum(X_train, axis=1) > 5

def create_tf_model():
   model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(10,)),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')

   ])
   model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])
   return model

tf_model = create_tf_model()

tf_history = tf_model.fit(X_train, y_train,
                         epochs=10,
                         batch_size=32,
                         validation_split=0.2,
                         verbose=0)

***************************************************
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

X_torch = torch.FloatTensor(X_train)
y_torch = torch.FloatTensor(y_train)

dataset = TensorDataset(X_torch, y_torch)

dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

class PyTorchModel(nn.Module):
    def __init__(self):
        super(PyTorchModel, self).__init__()
        self.layer1 = nn.Linear(10, 64)     #It takes 10 input features (the size of the input data) and transforms them into 64 output features (neurons in this layer).
        self.layer2 = nn.Linear(64, 32)      #This is typically used for binary classification tasks, where the model outputs a probability score
        self.layer3 = nn.Linear(32, 1)
        self.relu = nn.ReLU()     #ReLU is commonly used as an activation function in neural networks because it introduces non-linearity, allowing the model to learn complex patterns.
        self.dropout = nn.Dropout(0.2)   #keep 20% for validation
        self.sigmoid = nn.Sigmoid()   #This creates an instance of the Sigmoid activation function, which squashes the output to a range between 0 and 1.

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout(x)
        x = self.relu(self.layer2(x))
        x = self.sigmoid(self.layer3(x))
        return x

# Initialize model, loss function, and optimizer
torch_model = PyTorchModel()
criterion = nn.BCELoss()
optimizer = optim.Adam(torch_model.parameters())

# Training loop for PyTorch
def train_pytorch_model(model, dataloader, criterion, optimizer, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.unsqueeze(1))
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

# Train PyTorch model
train_pytorch_model(torch_model, dataloader, criterion, optimizer)

# Example predictions
def make_predictions():
    # Generate test data
    X_test = np.random.rand(5, 10)

    # TensorFlow predictions
    tf_preds = tf_model.predict(X_test)

    # PyTorch predictions
    torch_model.eval()
    with torch.no_grad():
        torch_preds = torch_model(torch.FloatTensor(X_test))

    return X_test, tf_preds, torch_preds.numpy()

# Make and display predictions
X_test, tf_preds, torch_preds = make_predictions()



_________________________________________________________________________________________________________________

----------------2 Diabetes-----------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import keras

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import SGD

df= pd.read_csv('/content/diabetes.csv')
df

df.head()

y= df['Outcome'].values
x= df.drop(columns=['Outcome']).values

sc= StandardScaler()
x= sc.fit_transform(x)

x_train , x_test , y_train , y_test = train_test_split(x,y , test_size=.25)

model = Sequential([
    layers.Dense(128, activation='relu', input_shape=(x.shape[1],)),
    Dropout(0.3),
    layers.Dense(64, activation='relu'),
    Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy' , metrics=['accuracy'] , optimizer='adam')

history =  model.fit(x_train, y_train , epochs=20,batch_size=32,validation_data=(x_test,y_test) , verbose=1)

plt.figure(figsize=(12, 4))

# plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()




_________________________________________________________________________________________________________________


---------------3 MNIST DIGIT---------------------

import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import random

from sklearn.model_selection import  train_test_split

#b load training and testing data(MNIST)
mnist = tf.keras.datasets.mnist #importing MNIST Dataset

(x_train,y_train),(x_test,y_test)=mnist.load_data()  #splitting into training and testing
#x_train = x_train / 255
#x_test = x_test / 255

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Check the shapes of the loaded data
print("x_train shape:", x_train.shape)  # Should be (60000, 28, 28)
print("y_train shape:", y_train.shape)  # Should be (60000,)
print("x_test shape:", x_test.shape)    # Should be (10000, 28, 28)
print("y_test shape:", y_test.shape)    # Should be (10000,)

#c Define network architechture using keras

model =keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'), #128 neurons i.e. hidden layer
    keras.layers.Dense(10,activation='softmax') #10 neurons i.e. output layer
])
model.summary()

# d. Train model using SGD
# Compile the model
model.compile(optimizer='sgd',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)

#e. evaluate the network
test_loss,test_acc=model.evaluate(x_test,y_test)
print('loss=%.3f'%test_loss)
print('accuracy=%.3f'%test_acc)

n=random.randint(0,9999)
plt.imshow(x_test[n])
plt.show()
predicted_value=model.predict(x_test)
plt.imshow(x_test[n])
plt.show()

print('predicted value:',predicted_value[n])

#f. plot training loss and accuracy

#accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train','Validation'],loc='upper right')
plt.show()

#loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train','validation'],loc='upper left')
plt.show()

___________________________________________________________________________________________________________________________

----------------4 CIFAR 10--------------------------

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import cifar10
import random

# Load CIFAR-10 dataset directly
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize the data
x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0

# Convert labels to one-hot encoded format
y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse']

model = Sequential([
 Flatten(input_shape=(32,32,3)), 
 Dense(128, activation='relu'),
 Dense(64, activation='relu'),
 Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=15, batch_size=64, validation_data=(x_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

plt.figure(figsize=(12, 4))
#plot loss
plt.subplot(1,2,1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','Test'],loc='upper right')


#plot accuracy
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','Test'],loc='upper right')
plt.show()

#plot one training image
plt.figure(figsize=(4,4))
plt.imshow(x_train[0])
plt.title(f'Train Image: {class_names[np.argmax(y_train[0])]}')
plt.axis('off')
plt.show()



#plot one testing img
n = random.randint(0, len(x_test) - 1)
plt.figure(figsize=(4, 4))
plt.imshow(x_test[n])
plt.title(f'Test Image: {class_names[np.argmax(y_test[n])]}')
plt.axis('off')
plt.show()

___________________________________________________________________________________________________________________________________________

------------5. FASHION IMAGE CLASSIFICATION----------------------

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# 1. Load and preprocess the Fashion MNIST dataset
(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()

# Add channel dimension and normalize pixel values
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255.0
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255.0

# Define class names for visualization
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# 2. Visualize some training examples
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(train_images[i].reshape(28, 28), cmap='gray')
    plt.title(class_names[train_labels[i]])
    plt.axis('off')
plt.show()

# 3. Create the CNN model
model = models.Sequential([
    # First Convolutional Block
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),

    # Second Convolutional Block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),

    # Third Convolutional Block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.25),

    # Flatten and Dense Layers
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Print model summary
model.summary()

# 4. Compile the model
model.compile(optimizer='adam',
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

# 5. Add callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=3,
        min_lr=1e-6
    )
]

# 6. Train the model
history = model.fit(train_images, train_labels,
                   batch_size=64,
                   epochs=30,
                   validation_split=0.2,
                   callbacks=callbacks,
                   verbose=1)

# 7. Plot training history
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# 8. Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)
print(f"\nTest accuracy: {test_accuracy:.4f}")
print(f"Test loss: {test_loss:.4f}")

# 9. Make predictions and create confusion matrix
predictions = model.predict(test_images)
pred_labels = np.argmax(predictions, axis=1)

# Create and plot confusion matrix
cm = confusion_matrix(test_labels, pred_labels)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.tight_layout()
plt.show()

# 10. Display some predictions
plt.figure(figsize=(12, 8))
for i in range(12):
    plt.subplot(3, 4, i + 1)
    plt.imshow(test_images[i].reshape(28, 28), cmap='gray')
    predicted = class_names[pred_labels[i]]
    actual = class_names[test_labels[i]]
    color = 'green' if predicted == actual else 'red'
    plt.title(f'Pred: {predicted}\nTrue: {actual}', color=color)
    plt.axis('off')
plt.tight_layout()
plt.show()

__________________________________________________________________________________________________________________________________________
------------------8. RNN------------------------------------

import numpy as np

# Define activation functions and their derivatives
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

def mse_loss(y_pred, y_true):
    return np.mean((y_pred - y_true)**2)

# Initialize the RNN parameters
input_size = 1      # Input feature size (e.g., a single value in a sequence)
hidden_size = 16    # Number of hidden units
output_size = 1     # Output size (single value prediction)
sequence_length = 10 # Length of each input sequence
learning_rate = 0.01

# Weight matrices
Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden
Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # Hidden to hidden
Why = np.random.randn(output_size, hidden_size) * 0.01 # Hidden to output

# Biases
bh = np.zeros((hidden_size, 1))
by = np.zeros((output_size, 1))


# Forward pass through the RNN for a sequence
def forward(x_seq):
    h_seq = np.zeros((sequence_length, hidden_size, 1))  # Stores hidden states
    y_seq = np.zeros((sequence_length, output_size, 1))  # Stores outputs

    for t in range(sequence_length):
        x = x_seq[t].reshape(-1, 1)  # Get current time step input and reshape for matrix operations
        h_prev = h_seq[t - 1] if t > 0 else np.zeros((hidden_size, 1))  # Use previous hidden state
        h = tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)  # Compute hidden state
        y = np.dot(Why, h) + by                               # Compute output

        h_seq[t] = h  # Save hidden state
        y_seq[t] = y  # Save output

    return y_seq, h_seq

# Backpropagation Through Time (BPTT) for the RNN
def backward(x_seq, y_true_seq, y_seq, h_seq):
    global Wxh, Whh, Why, bh, by

    # Gradients initialization
    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
    dbh, dby = np.zeros_like(bh), np.zeros_like(by)
    dh_next = np.zeros((hidden_size, 1))  # Gradient of the next time step's hidden state

    for t in reversed(range(sequence_length)):
        dy = y_seq[t] - y_true_seq[t].reshape(-1, 1)  # Output error
        dWhy += np.dot(dy, h_seq[t].T)
        dby += dy

        dh = np.dot(Why.T, dy) + dh_next  # Backpropagate into hidden state
        dh_raw = dh * tanh_derivative(h_seq[t])  # Apply tanh derivative
        dbh += dh_raw
        dWxh += np.dot(dh_raw, x_seq[t].reshape(1, -1))
        if t > 0:
            dWhh += np.dot(dh_raw, h_seq[t - 1].T)
        dh_next = np.dot(Whh.T, dh_raw)

    # Update weights and biases using gradient descent
    for param, dparam in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby]):
        param -= learning_rate * dparam

# Training loop
epochs = 100
for epoch in range(epochs):
    # Generate a dummy input sequence and target
    x_seq = np.sin(np.linspace(0, 2 * np.pi, sequence_length))  # Example sequence (sinusoidal)
    y_true_seq = np.roll(x_seq, -1)  # Target is the sequence shifted by one step
    x_seq = x_seq.reshape(sequence_length, 1)  # Reshape for processing
    y_true_seq = y_true_seq.reshape(sequence_length, 1)

    # Forward pass
    y_seq, h_seq = forward(x_seq)

    # Calculate and print loss
    loss = mse_loss(y_seq, y_true_seq)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss}")

    # Backward pass
    backward(x_seq, y_true_seq, y_seq, h_seq)


___________________________________________________________________________________________________________________________________________

-------------------9 Credit Card-------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras.datasets import mnist
from keras import layers, regularizers
from keras import Model
from keras.optimizers import Adam
from keras.layers import Dense, MaxPooling2D, GlobalAveragePooling2D

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score,precision_score,recall_score
from sklearn.model_selection import train_test_split


df= pd.read_csv('/content/creditcard.csv')

print('any null values',df.isnull().values.any())
print('labels',len(df['Class'].unique()))
print('label values',df.Class.unique())

count_classes=pd.value_counts(df['Class'], sort=True)
count_classes.plot(kind='bar',rot=0)
plt.show()

normal_data=df[df['Class']==0]
fraud_data=df[df['Class']==1]
print('normal data=',normal_data.shape)
print('fraud_data=',fraud_data.shape)

bins= np.linspace(200,2500,100)
plt.hist(normal_data['Amount'], bins=bins,color='g',alpha=1,density='True',label='Normal')
plt.hist(fraud_data['Amount'],bins=bins,color='r',alpha=0.5,density='True',label='fraud')
plt.show()

sc= StandardScaler()
df['Amount']=sc.fit_transform(df['Amount'].values.reshape(-1,1))
df['Time']=sc.fit_transform(df['Time'].values.reshape(-1,1))

raw=df.values
labels= raw[:, -1]
data= raw[:, :-1]

train_data,test_data,train_labels,test_labels=train_test_split(data,labels,test_size=0.2, random_state=2021)

train_labels=train_labels.astype(bool)
test_labels=test_labels.astype(bool)

min_val=tf.reduce_min(train_data)
max_val=tf.reduce_max(train_data)
train_data=(train_data-min_val)/(max_val-min_val)
test_data=(test_data-min_val)/(max_val-min_val)

normal_train_data= train_data[~train_labels]
fraud_train_data= train_data[train_labels]

input_dim= normal_train_data.shape[1]
input_layer= tf.keras.layers.Input(shape=(input_dim,))

encoder=tf.keras.layers.Dense(7,activation='tanh')(input_layer)
encoder=tf.keras.layers.Dropout(0.2)(encoder)
encoder=tf.keras.layers.Dense(7,activation='relu')(encoder)
encoder=tf.keras.layers.Dense(4,activation=tf.nn.leaky_relu)(encoder)

decoder=tf.keras.layers.Dense(7,activation='relu')(encoder)
decoder=tf.keras.layers.Dropout(0.2)(decoder)
decoder=tf.keras.layers.Dense(input_dim,activation='tanh')(decoder)

autoencoder= tf.keras.models.Model(inputs=input_layer,outputs=decoder)

autoencoder.compile(optimizer='adam',metrics=['accuracy'],loss='mae')

history= autoencoder.fit(normal_train_data,normal_train_data,
                         validation_data=(test_data,test_data),
                         epochs=10,
                         batch_size=64,
                         shuffle=True
                         ).history

test_x_prediction = autoencoder.predict(test_data)
test_loss = tf.keras.losses.mse(test_data, test_x_prediction)
error_df = pd.DataFrame({'Reconstruction_error': test_loss, 'True_class': test_labels})

threshold_fixed = np.percentile(error_df[error_df['True_class'] == 0].Reconstruction_error, 95)

# Error threshold plot
#threshold_fixed = 0.003
#groups = error_df.groupby('True_class')
#fig, ax = plt.subplots()
#for name, group in groups:
    #ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',
            #label="Fraud" if name == 1 else "Normal")
#ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
#ax.legend()
#plt.title("Reconstruction error for normal and fraud data points")
#plt.ylabel("Reconstruction error")
#plt.xlabel("Data point index")
#plt.show()

pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df['True_class'], pred_y)
sns.heatmap(conf_matrix, cmap='Blues', annot=True, fmt='d')
plt.title('Confusion matrix')
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()

# Metrics
print("Accuracy:", accuracy_score(error_df['True_class'], pred_y))
print("Recall:", recall_score(error_df['True_class'], pred_y))
print("Precision:", precision_score(error_df['True_class'], pred_y))

______________________________________________________________________________________________________________________________

-------------------10 DENOISING-----------------------------

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

def add_noise(images):
    noise_factor = 0.5
    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
    return np.clip(noisy_images, 0., 1.)
x_train_noisy = add_noise(x_train)
x_test_noisy = add_noise(x_test)

def build_autoencoder():
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))
    model.add(layers.MaxPooling2D((2, 2), padding='same'))
    model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), padding='same'))
    model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    model.add(layers.UpSampling2D((2, 2)))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    model.add(layers.UpSampling2D((2, 2)))
    model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
    return model

autoencoder = build_autoencoder()
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train_noisy, x_train, epochs=20, batch_size=128, validation_data=(x_test_noisy, x_test))

x_test_denoised = autoencoder.predict(x_test_noisy)

n = 10
plt.figure(figsize=(20, 6))
for i in range(n):
    ax = plt.subplot(3, n, i + 1)
    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')
    plt.title("Noisy Image")
    plt.axis('off')

    ax = plt.subplot(3, n, i + 1 + n)
    plt.imshow(x_test_denoised[i].reshape(28, 28), cmap='gray')
    plt.title("Denoised Image")
    plt.axis('off')

    ax = plt.subplot(3, n, i + 1 + 2 * n)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title("Original Image")
    plt.axis('off')

plt.show()

____________________________________________________________________________________________________________________________________________________

-------------13. BAG OF WORDS---------------
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Embedding, Dense, Lambda, Input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import io

# a. Data preparation
def load_data():
    # For this example, we'll use a small corpus
    corpus = [
        "the quick brown fox jumps over the lazy dog",
        "the five boxing wizards jump quickly",
        "how vexingly quick daft zebras jump",
        "the jay pig fox zebra and my wolves quack",
        "sphinx of black quartz judge my vow"
    ]
    return corpus

def prepare_data(corpus):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(corpus)
    V = len(tokenizer.word_index) + 1
    sequences = tokenizer.texts_to_sequences(corpus)
    return sequences, V, tokenizer

 #b. Generate training data
def generate_training_data(sequences, window_size, V):
    X = []
    y = []
    for sequence in sequences:
        for i in range(len(sequence)):
            context_start = max(0, i - window_size)
            context_end = min(len(sequence), i + window_size + 1)
            context = sequence[context_start:i] + sequence[i+1:context_end]
            if len(context) > 0:
                X.append(context)
                y.append(sequence[i])
    X = pad_sequences(X, maxlen=window_size*2, padding='pre')
    y = np.array(y)
    return X, y

# c. Train model
def create_model(V, embedding_dim, max_context_length):
    input_context = Input(shape=(max_context_length,))

    embedding = Embedding(V, embedding_dim, input_length=max_context_length, name='embedding')
    context_embedding = embedding(input_context)

    context_vector = Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_embedding)

    output = Dense(V, activation='softmax')(context_vector)

    model = Model(inputs=input_context, outputs=output)
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

    return model

def train_model(model, X, y, epochs=50, batch_size=32):
    history = model.fit(
        X, y,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.2,
        verbose=1
    )
    return history

# d. Output
def get_embeddings(model, tokenizer):
    weights = model.get_layer('embedding').get_weights()[0]
    reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])
    word_embeddings = {reverse_word_index[i]: weights[i] for i in range(1, len(reverse_word_index) + 1)}
    return word_embeddings

def find_similar_words(word, word_embeddings, n=5):
    target_embedding = word_embeddings[word]
    similarities = {}
    for w, emb in word_embeddings.items():
        if w != word:
            similarity = np.dot(target_embedding, emb) / (np.linalg.norm(target_embedding) * np.linalg.norm(emb))
            similarities[w] = similarity
    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n]
     
# Main execution
if __name__ == "__main__":
    # a. Data preparation
    corpus = load_data()
    sequences, V, tokenizer = prepare_data(corpus)

    # b. Generate training data
    window_size = 2
    X, y = generate_training_data(sequences, window_size, V)

    # c. Train model
    embedding_dim = 100
    max_context_length = window_size * 2
    model = create_model(V, embedding_dim, max_context_length)
    history = train_model(model, X, y)

    # d. Output
    word_embeddings = get_embeddings(model, tokenizer)

# Print some similar words
test_words = ['quick', 'fox', 'jump']
for word in test_words:
        print(f"\nWords similar to '{word}':")
        similar_words = find_similar_words(word, word_embeddings)
        for similar_word, similarity in similar_words:
            print(f"{similar_word}: {similarity:.4f}")

     

# Plot training history
import matplotlib.pyplot as plt
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()




